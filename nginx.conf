# ============================================================================
# NGINX COMPLETE CONFIGURATION WITH DETAILED EXPLANATIONS
# ============================================================================

# MAIN CONTEXT (Global Settings - affects entire nginx instance)
# ----------------------------------------------------------------------------
worker_processes auto;
# - Determines how many worker processes nginx will create
# - 'auto' = nginx automatically detects CPU cores and creates 1 worker per core
# - Alternative: Set manually like 'worker_processes 4;' for 4 workers
# - More workers = better handling of concurrent requests
# - Rule: Usually 1 worker per CPU core is optimal

# EVENTS BLOCK (Connection handling configuration)
# ----------------------------------------------------------------------------
events {
    # Maximum simultaneous connections each worker process can handle
    worker_connections 1024;
    # - Each worker can handle 1024 concurrent connections
    # - Total server capacity = worker_processes × worker_connections
    # - Example: 4 workers × 1024 = 4,096 total concurrent connections
    # - Increase this for high-traffic sites (common values: 1024, 2048, 4096)

    # Event processing method - how nginx handles I/O operations
    use epoll;
    # - 'epoll' is Linux's most efficient event notification mechanism
    # - Alternative methods: select, poll, kqueue (for BSD/macOS)
    # - epoll scales better with large numbers of connections
    # - nginx auto-selects best method if not specified
}

# HTTP BLOCK (All HTTP-related configuration)
# ----------------------------------------------------------------------------
http {

    # CONNECTION KEEP-ALIVE SETTINGS
    # ------------------------------------------------------------------------
    keepalive_timeout 65;
    # - How long (in seconds) to keep client connections open for reuse
    # - Benefit: Client can send multiple requests over same connection
    # - Reduces overhead of opening/closing connections
    # - Too high = wastes memory on idle connections
    # - Too low = more CPU usage creating new connections
    # - 65 seconds is a balanced default

    # FILE UPLOAD LIMITS
    # ------------------------------------------------------------------------
    client_max_body_size 10M;
    # - Maximum size allowed for client request body (includes file uploads)
    # - Default is only 1MB, which blocks most file uploads
    # - 10M = allows files up to 10 megabytes
    # - Adjust based on your application needs (1M, 50M, 100M, etc.)
    # - Prevents denial of service attacks with huge uploads

    # LOAD BALANCER CONFIGURATION
    # ------------------------------------------------------------------------
    upstream backend {
        # Define a group of backend servers for load balancing
        # 'backend' is the name we'll reference in proxy_pass

        server app1:8080;  # First Spring Boot application instance
        server app2:8080;  # Second Spring Boot application instance
        server app3:8080;  # Third Spring Boot application instance

        # Load balancing method: Round-robin (default)
        # - Request 1 goes to app1
        # - Request 2 goes to app2
        # - Request 3 goes to app3
        # - Request 4 goes back to app1, and so on...
        # - This distributes load evenly across all servers
    }

    # VIRTUAL SERVER CONFIGURATION
    # ------------------------------------------------------------------------
    server {
        # This server block defines how nginx handles incoming requests

        # LISTENING CONFIGURATION
        listen 80;
        # - nginx listens on port 80 (standard HTTP port)
        # - All HTTP requests to this server will be handled here
        # - Could also specify IP: 'listen 192.168.1.100:80;'

        # LOCATION BLOCK - URL routing rules
        location / {
            # This handles all requests that start with '/' (which is everything)
            # More specific locations (like /api/, /static/) would be checked first

            # REVERSE PROXY CONFIGURATION
            proxy_pass http://backend;
            # - Forward incoming requests to the 'backend' upstream group
            # - nginx acts as reverse proxy, hiding backend servers from clients
            # - 'backend' refers to our upstream block defined above
            # - nginx will load balance between app1, app2, app3

            # PROXY HEADERS - Pass client information to backend servers
            proxy_set_header Host $host;
            # - Preserves the original Host header from client request
            # - Backend servers can see what domain was requested
            # - Important for applications serving multiple domains

            proxy_set_header X-Real-IP $remote_addr;
            # - Passes the real client IP address to backend servers
            # - Without this, backend only sees nginx's IP address
            # - Useful for logging, security, geolocation features
            # - $remote_addr is nginx variable containing client's IP
        }
    }
}

# ============================================================================
# PERFORMANCE IMPACT OF THIS CONFIGURATION:
# ============================================================================
#
# Capacity: Can handle thousands of concurrent connections
# Efficiency: Reuses connections, reduces connection overhead
# Scalability: Distributes load across multiple application instances
# Reliability: If one app fails, others continue serving requests
# Flexibility: Easy to add/remove backend servers
#
# TESTING THIS CONFIGURATION:
# 1. Run: docker compose up
# 2. Visit: http://localhost - see different app instances respond
# 3. Refresh multiple times - observe round-robin load balancing
# 4. Monitor docker logs to see which containers handle requests
# ============================================================================
